{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from docx import Document\n",
        "from docx.shared import Inches, Pt\n",
        "import time\n",
        "\n",
        "# Function to get the current year and week\n",
        "def get_year_and_week():\n",
        "    \"\"\"Return the current year and week of the year as strings.\"\"\"\n",
        "    now = datetime.now()\n",
        "    year = now.strftime(\"%Y\")         # Current year\n",
        "    week_of_year = now.strftime(\"%U\") # Week number of the year\n",
        "    return year, week_of_year\n",
        "\n",
        "# Function to fetch new videos from a YouTube channel without using the API\n",
        "def fetch_new_videos_scrape(channel_id):\n",
        "    start_time = time.time()\n",
        "    url = f\"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Failed to fetch video feed.\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'xml')\n",
        "    videos = []\n",
        "    one_week_ago = (datetime.utcnow() - timedelta(days=7)).replace(tzinfo=timezone.utc)  # Make timezone-aware\n",
        "\n",
        "    channel_title = soup.find('title').text\n",
        "    channel_logo_url = soup.find('logo').text if soup.find('logo') else None\n",
        "\n",
        "    for entry in soup.find_all('entry'):\n",
        "        video_published = datetime.strptime(entry.published.text, \"%Y-%m-%dT%H:%M:%S%z\")\n",
        "        if video_published > one_week_ago:\n",
        "            high_quality_thumbnail = entry.find('media:thumbnail')['url'].replace('hqdefault.jpg', 'maxresdefault.jpg')\n",
        "            video = {\n",
        "                'title': entry.title.text,\n",
        "                'link': entry.link['href'],\n",
        "                'thumbnail': high_quality_thumbnail,\n",
        "                'published': video_published.strftime('%Y-%m-%d %H:%M:%S UTC'),\n",
        "                'description': entry.find('media:description').text if entry.find('media:description') else \"No description available.\"\n",
        "            }\n",
        "            videos.append(video)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Scraping completed in {elapsed_time:.2f} seconds with {len(videos)} videos fetched.\")\n",
        "    return videos, elapsed_time, channel_title, channel_logo_url\n",
        "\n",
        "# Function to create a Word document with video details\n",
        "def create_word_doc(videos, elapsed_time, output_path, channel_id, channel_title, channel_logo_url):\n",
        "    document = Document()\n",
        "\n",
        "    # Add a sci-fi styled title\n",
        "    title = document.add_heading(level=1)\n",
        "    run = title.add_run('YouTube Weekly Video Report')\n",
        "    run.font.name = 'Orbitron'  # Sci-fi font (use a similar available font if Orbitron isn't installed)\n",
        "    run.font.size = Pt(24)\n",
        "\n",
        "    # Channel details\n",
        "    if channel_logo_url:\n",
        "        response = requests.get(channel_logo_url)\n",
        "        if response.status_code == 200:\n",
        "            logo_path = os.path.join(output_path, \"channel_logo.jpg\")\n",
        "            with open(logo_path, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            document.add_picture(logo_path, width=Inches(1.5))\n",
        "            os.remove(logo_path)\n",
        "\n",
        "    document.add_heading(channel_title, level=2)\n",
        "    document.add_paragraph(f\"Channel ID: {channel_id}\")\n",
        "    document.add_paragraph(f\"Report Generated On: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n",
        "    document.add_paragraph(f\"Time Taken to Fetch Videos: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "    # Add a separator\n",
        "    separator = document.add_paragraph()\n",
        "    separator_run = separator.add_run(\"=\" * 50)\n",
        "    separator_run.bold = True\n",
        "\n",
        "    for idx, video in enumerate(videos, 1):\n",
        "        document.add_heading(f\"{idx}. {video['title']}\", level=2)\n",
        "        document.add_paragraph(f\"Video Link: {video['link']}\")\n",
        "        document.add_paragraph(f\"Published On: {video['published']}\")\n",
        "        document.add_paragraph(f\"Description: {video['description']}\")\n",
        "\n",
        "        # Fetch and insert the high-quality thumbnail\n",
        "        response = requests.get(video['thumbnail'])\n",
        "        if response.status_code == 200:\n",
        "            thumbnail_path = os.path.join(output_path, f\"temp_thumbnail_{idx}.jpg\")\n",
        "            with open(thumbnail_path, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            document.add_picture(thumbnail_path, width=Inches(3))\n",
        "            os.remove(thumbnail_path)\n",
        "\n",
        "        # Add a separator between videos\n",
        "        separator = document.add_paragraph()\n",
        "        separator_run = separator.add_run(\"-\" * 50)\n",
        "        separator_run.bold = True\n",
        "\n",
        "    # Add logs at the end\n",
        "    document.add_page_break()\n",
        "    document.add_heading(\"Debug Insights\", level=1)\n",
        "    document.add_paragraph(f\"Total Videos Fetched: {len(videos)}\")\n",
        "    document.add_paragraph(f\"Time Taken: {elapsed_time:.2f} seconds\")\n",
        "    document.add_paragraph(f\"Channel: {channel_title}\")\n",
        "    document.add_paragraph(f\"Channel ID: {channel_id}\")\n",
        "\n",
        "    # Save the Word document with the updated naming convention\n",
        "    year, week = get_year_and_week()\n",
        "    doc_name = f\"{year}-{week}-YouTube_Videos_Report.docx\"\n",
        "    doc_path = os.path.join(output_path, doc_name)\n",
        "    document.save(doc_path)\n",
        "    print(f\"Document saved at: {doc_path}\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    channel_id = 'UCeoSGHaFePbHGtJjn0xUPrw'  # Your Channel ID\n",
        "    output_path = '.'  # Current directory\n",
        "\n",
        "    videos, elapsed_time, channel_title, channel_logo_url = fetch_new_videos_scrape(channel_id)\n",
        "    if videos:\n",
        "        create_word_doc(videos, elapsed_time, output_path, channel_id, channel_title, channel_logo_url)\n",
        "    else:\n",
        "        print(\"No new videos found for the past week.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuVYFUM4_xtJ",
        "outputId": "ac70e9c6-6b15-42b3-b83d-284f889165ee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping completed in 0.17 seconds with 1 videos fetched.\n",
            "Document saved at: ./2024-50-YouTube_Videos_Report.docx\n"
          ]
        }
      ]
    }
  ]
}